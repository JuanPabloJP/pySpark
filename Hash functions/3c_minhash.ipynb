{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c_minhash.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIbp5TpPRjSOdJagQkqgba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoDatosMasivosI/blob/main/notebooks/3c_minhash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kloh2BluGySD"
      },
      "source": [
        "# Búsqueda de documentos con MinHash\n",
        "En esta libreta veremos cómo hacer búsqueda eficiente de documentos similares considerando la similitud de Jaccard usando MinHash.\n",
        "\n",
        "Primero cargamos los módulos necesarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzaNGg6PA6_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor \n",
        "\n",
        "import random\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_56-act4ikel"
      },
      "source": [
        "Vamos a usar el conjunto de documentos de _20 Newsgropus_, el cual descargamos usando scikit-learn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RLivGmPgIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00817d1e-733c-4ed3-a1c7-787db80442ca"
      },
      "source": [
        "db = fetch_20newsgroups(remove=('headers','footers','quotes'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alui_Ua7iva9"
      },
      "source": [
        "Definimos nuestro analizador léxico usando la biblioteca NLTK. Vamos a extraer los componentes léxicos, pasarlos a minúsculas y lematizarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnA4Yn-HPLQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393c7a7c-a7cf-4fd8-c1d7-eba7941a0a01"
      },
      "source": [
        "import nltk\n",
        "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADV, ADJ\n",
        "\n",
        "morphy_tag = {\n",
        "    'JJ' : ADJ,\n",
        "    'JJR' : ADJ,\n",
        "    'JJS' : ADJ,\n",
        "    'VB' : VERB,\n",
        "    'VBD' : VERB,\n",
        "    'VBG' : VERB,\n",
        "    'VBN' : VERB,\n",
        "    'VBP' : VERB,\n",
        "    'VBZ' : VERB,\n",
        "    'RB' : ADV,\n",
        "    'RBR' : ADV,\n",
        "    'RBS' : ADV\n",
        "}\n",
        "\n",
        "def doc_a_tokens(doc):\n",
        "  tagged = pos_tag(word_tokenize(doc.lower()))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = []\n",
        "  for p,t in tagged:\n",
        "    tokens.append(lemmatizer.lemmatize(p, pos=morphy_tag.get(t, NOUN)))\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ado-nD45jFNP"
      },
      "source": [
        "Dividimos nuestro conjunto en 2 subconjuntos: los documentos de la base que se buscarán y los documentos de consulta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduiS-ukgm7m"
      },
      "source": [
        "n = len(db.data)\n",
        "perm = np.random.permutation(n).astype(int)\n",
        "n_ej = int(floor(n * 0.95))\n",
        "\n",
        "base = [db.data[i] for i in perm[:n_ej]]\n",
        "consultas = [db.data[i] for i in perm[n_ej:]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOpgvsMJjYsf"
      },
      "source": [
        "Calculamos las bolsas de palabras del conjunto base usando la clase `CountVectorizer` de scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxFG3nyyPOVs"
      },
      "source": [
        "docs_base = []\n",
        "for d in base:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  tokens = doc_a_tokens(d)\n",
        "  docs_base.append(' '.join(tokens))\n",
        "v = CountVectorizer(stop_words='english', max_features=5000, max_df=0.8)\n",
        "bolsas_base = v.fit_transform(docs_base)\n",
        "\n",
        "dim = bolsas_base.shape[1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtNlXvCgjlE2"
      },
      "source": [
        "También calculamos las bolsas para las consultas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtaZjNQuhgs9"
      },
      "source": [
        "docs_consultas = []\n",
        "for d in consultas:\n",
        "  d = d.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  tokens = doc_a_tokens(d)\n",
        "  docs_consultas.append(' '.join(tokens))\n",
        "\n",
        "bolsas_consultas = v.transform(docs_consultas)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejYh-CijqkP"
      },
      "source": [
        "Finalmente, definimos nuestra clase para MinHash, la cual encapsula las funciones para calcular los valores MinHash, las tuplas y los índices, la tabla y las operaciones de inserción, búsqueda y eliminación sobre esta. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQR1PIsnyCPt"
      },
      "source": [
        "class MinHashTable:\n",
        "  def __init__(self, n_cubetas, t_tupla, dim):\n",
        "    self.n_cubetas = n_cubetas\n",
        "    self.tabla = [[] for i in range(n_cubetas)]\n",
        "    self.dim = dim\n",
        "    self.t_tupla = t_tupla\n",
        "    self.perm = np.random.randint(0, np.iinfo(np.int32).max, size=(self.dim, self.t_tupla))\n",
        "    self.rind = np.random.randint(0, np.iinfo(np.int32).max, size=(self.dim, self.t_tupla))\n",
        "    self.a = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.b = np.random.randint(0, np.iinfo(np.int32).max, size=self.t_tupla)\n",
        "    self.primo = 4294967291\n",
        "\n",
        "  def __repr__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas)]\n",
        "    return \"<TablaHash :%s >\" % ('\\n'.join(contenido))\n",
        "\n",
        "  def __str__(self):\n",
        "    contenido = ['%d::%s' % (i, self.tabla[i]) for i in range(self.n_cubetas) if self.tabla[i]]\n",
        "    return '\\n'.join(contenido)\n",
        "\n",
        "  def sl(self, x, i):\n",
        "    return (self.h(x) + i) % self.n_cubetas\n",
        "\n",
        "  def h(self, x):\n",
        "    return x % self.primo\n",
        "\n",
        "  def minhash(self, x):\n",
        "    xp = self.perm[x]\n",
        "    xi = self.rind[x]\n",
        "    amin = xp.argmin(axis = 0)\n",
        "    \n",
        "    pmin = xp[amin, np.arange(0, self.t_tupla)]\n",
        "    emin = xi[amin, np.arange(0, self.t_tupla)]\n",
        "\n",
        "    return np.sum(self.a * pmin, dtype=np.ulonglong), np.sum(self.b * emin, dtype=np.ulonglong)\n",
        "     \n",
        "  def insertar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "  \n",
        "    llena = True\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        self.tabla[cubeta].append(mh)\n",
        "        self.tabla[cubeta].append([ident])\n",
        "        llena = False\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        self.tabla[cubeta][1].append(ident)\n",
        "        llena = False\n",
        "        break\n",
        "\n",
        "    if llena:\n",
        "      print('¡Error, tabla llena!')\n",
        "\n",
        "  def buscar(self, x):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        return []\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1]\n",
        "        \n",
        "    return []\n",
        "\n",
        "  def eliminar(self, x, ident):\n",
        "    mh, v2 = self.minhash(x)\n",
        "\n",
        "    for i in range(self.n_cubetas):\n",
        "      cubeta = int(self.sl(v2, i))\n",
        "      if not self.tabla[cubeta]:\n",
        "        break\n",
        "      elif self.tabla[cubeta][0] == mh:\n",
        "        return self.tabla[cubeta][1].remove(ident)\n",
        "\n",
        "    return -1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAaoXELkJzX"
      },
      "source": [
        "Ahora instanciamos esta clase tantas veces como tablas queramos para realizar la búsqueda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7gNvp8yO0nF"
      },
      "source": [
        "n_tablas = 10\n",
        "tablas = [MinHashTable(2**21, 3, dim) for _ in range(n_tablas)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNuWULYQkZ2z"
      },
      "source": [
        "Definimos una función para convertir de matriz dispersa tipo CSR a una lista de listas. Nota que no se están considerando las frecuencias de las bolsas, por lo que la representación del documento es un conjunto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lAXGApOspb"
      },
      "source": [
        "def csr_to_ldb(csr):\n",
        "  ldb = [[] for _ in range(csr.shape[0])]\n",
        "  coo = csr.tocoo()    \n",
        "  for i,j,v in zip(coo.row, coo.col, coo.data):\n",
        "    ldb[i].append(j)\n",
        "\n",
        "  return ldb"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0yfAA8Yyba"
      },
      "source": [
        "ll_base = csr_to_ldb(bolsas_base)\n",
        "for j,l in enumerate(ll_base):\n",
        "    if l:\n",
        "      for i in range(n_tablas):\n",
        "        tablas[i].insertar(l, j)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylYmsJdqk1AL"
      },
      "source": [
        "Recuperamos los documentos similares a nuestros documentos de consulta usando las tablas MinHash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkSVxARjwZh"
      },
      "source": [
        "ll_consultas = csr_to_ldb(bolsas_consultas)\n",
        "docs = []\n",
        "for j,l in enumerate(ll_consultas):\n",
        "  dc = []\n",
        "  if l:\n",
        "    for i in range(n_tablas):\n",
        "      dc.extend(tablas[i].buscar(l))\n",
        "  docs.append(set(dc))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmyOkNBIlCXV"
      },
      "source": [
        "Finalmente, calculamos la similitud Jaccard de los documentos recuperados con los de consulta y los ordenamos por similitud.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDS0K4M6VXT-"
      },
      "source": [
        "def similitud_jaccard(x, y):\n",
        "  x = x.toarray()[0]\n",
        "  y = y.toarray()[0]\n",
        "  inter = np.count_nonzero(x * y)\n",
        "  return inter / (np.count_nonzero(x) + np.count_nonzero(y) - inter)\n",
        "\n",
        "def fuerza_bruta(ds, qs, fd):\n",
        "  medidas = np.zeros(ds.shape[0])\n",
        "  for i,x in enumerate(ds):\n",
        "    medidas[i] = fd(qs, x)\n",
        "\n",
        "  return np.sort(medidas)[::-1], np.argsort(medidas)[::-1]\n",
        "\n",
        "sims = []\n",
        "orden = []\n",
        "for i,q in enumerate(bolsas_consultas):\n",
        "  ld = list(docs[i])\n",
        "  if ld:\n",
        "    s,o = fuerza_bruta(bolsas_base[ld], q, similitud_jaccard)\n",
        "    sims.append(s)\n",
        "    orden.append([ld[e] for e in o])\n",
        "  else:\n",
        "    sims.append([])\n",
        "    orden.append([])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebzJclVJ96VW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b4e2b5-952b-49f7-cd1c-d2e62f56d252"
      },
      "source": [
        "print(sims[21])\n",
        "print(orden[21])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08974359 0.0862069  0.08       0.05825243 0.05325444 0.04444444\n",
            " 0.03726708 0.03571429]\n",
            "[3940, 1488, 5102, 1638, 2092, 5009, 7836, 6222]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnT6Db27lU-d"
      },
      "source": [
        "Examinamos los documentos más similares a uno de los de consulta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ePYSjvCoH4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad119260-a504-4f86-bf40-ef38a32eb927"
      },
      "source": [
        "print(consultas[21])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detroit is a very disciplined team.  There's a lot of Europeans\n",
            "in Detroit which would make the game fast, so Toronto would have\n",
            "to slow the game down, which means drawing penalties, as a last\n",
            "resort anyway.  Toronto will be a good team as soon as they get\n",
            "more good players.  Toronto is just an average team, Detroit isn't\n",
            "Ballard screwed Toronto when he was owner.  Everyone knows that.\n",
            "and it's going to take time for Toronto to become a real force.\n",
            "I expect Gilmour to be burnt out next year.  He can't pull the\n",
            "whole team forever.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTUDgDdgob0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8dcff0-3bdd-46c5-c7bb-a5cfaa188c3e"
      },
      "source": [
        "print(base[list(orden[21])[0]])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Yes. Reasonable parallels. (though I don't think Russia ever claimed to be\n",
            "Communist)\n",
            "\n",
            "\n",
            "I must protest your \"...in a Communist country\". How do you know?\n",
            "There haven't been any, and are unlikely to ever be any. In some Socialist\n",
            "dictatorships, you can't, whilst in some socialist democracies\n",
            "(such as France or Australia)\n",
            "you can. Of course, some people may disagree about France & Australia being\n",
            "socialist...\n",
            "\n",
            "\n",
            "Yet.\n",
            "\n",
            "\n",
            "In some circumstances. I was at a public meeting last night (in the USA), where\n",
            "a protester, who was very nice and calm, and just said before the\n",
            "speaker started to beware of his opinions, was forced out of the meeting by\n",
            "two armed policemen.\n",
            "\n",
            "There are a lot of things that one cannot do in the USA. You may not\n",
            "notice them, but as an Australian visitor, I notice them.\n",
            "\n",
            "\n",
            "Yes, we are lucky at the moment. I hope that is still true in\n",
            "a few years time. Because it didn't just happen...it required concious\n",
            "effort.\n",
            "\n",
            "\n",
            "Of course don't over react --- but don't under react.\n",
            "\n",
            "Andrew.\n",
            "Disclaimer: All my opinions are my own, and do not represent the society\n",
            "for the conservation of momentum or any other group. I hope I don't lose\n",
            "my student Visa as a result of these opinions..\n",
            "\n",
            "\n",
            "-- \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvxCNshmnMDh"
      },
      "source": [
        "## Ejercicio\n",
        "+ Evalúa la búsqueda con distintos valores de $r$ y $\\eta$ usando la fórmula de \n",
        "$$\n",
        "  l = \\frac{log(0.5)}{log(1 - \\eta^r)}\n",
        "$$\n",
        "\n",
        "+ Verifica que las colisiones de los conjuntos aproximan la similitud de Jaccard.\n",
        "+ Extiende la clase `MinHashTable` para que tome en cuenta las multiplicidades de las bolsas."
      ]
    }
  ]
}